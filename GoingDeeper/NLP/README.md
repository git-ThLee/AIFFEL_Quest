Contents from GoingDeeper, NLP progress.
##CONTENTS
|N|Title|Contents|
|------|------|------|
|1|텍스트 데이터 다루기 <br>Handling Text Data | 다양한 텍스트 데이터 전처리 기법을 소개, Word나 형태소 레벨의 tokenizer 및 subword 레벨 tokenizing 기법(BPE, sentencepiece) 학습 <br>Introducing various text data preprocessing techniques, including tokenizers at the word or morpheme level, and subword-level tokenizing techniques (BPE, sentencepiece)|
|2|멋진 단어사전 만들기 <br>Creating an Impressive Vocabulary | [PROJECT] 단어사전을 만들어보고 이를 토대로 perplexity를 측정해보는 프로젝트 <br>Create a vocabulary and measure perplexity based on it as a project|
|3|텍스트의 분포로 벡터화 하기 <br>Vectorizing Text Based on Distribution | 텍스트 분포를 이용한 텍스트의 벡터화 방법들(BoW, DTM, TF-IDF, LSA, LDA)<br>Methods for vectorizing text using text distributions (BoW, DTM, TF-IDF, LSA, LDA)|
|4|뉴스 카테고리 다중분류 <br>News Category Multi-Classification | [PROJECT] 뉴스 텍스트의 주제를 분류하는 task를 다양한 기법으로 시도해보고 비교, 분석 하는 프로젝트 <br>[PROJECT] Attempting and comparing various techniques for classifying the topics of news text, with analysis, as a project|
|5|워드 임베딩 <br>Word Embeddings | 워드 임베딩 벡터(Word2Vec, FastText, Glove)의 원리와 사용법을 학습 <br>Learning the principles and usage of word embedding vectors (Word2Vec, FastText, Glove)|
|6|WEAT | [PROJECT] WEAT(Word Embedding Association Test) 기법으로, Word Embedding Model 의 편향성 측정 <br>Measuring the bias of Word Embedding Models using the WEAT (Word Embedding Association Test) technique|
|7|Seq2seq와 Attention <br>Seq2seq and Attention | 언어 모델이 발전해 온 과정에 대해 배우고, Seq2seq에 대해 학습 <br>Learning about the evolution of language models and understanding Seq2seq|
|8|Seq2seq으로 번역기 만들기 <br>Creating a Translator with Seq2seq | Attention 기법을 추가하여 Seq2seq 기반의 번역기 성능을 높여보기 <br>Improving the performance of a Seq2seq-based translator by adding Attention|
|9|Transformer가 나오기까지 <br>Until the Emergence of Transformers | Attention 복습 및 트랜스포머에 포함된 모듈을 심층적으로 이해하는 단계 <br>Reviewing Attention and deeply understanding the modules included in Transformers|
|10|Transformer로 번역기 만들기 <br>Creating a Translator with Transformers | 트랜스포머를 이용해 번역기를 만드는 프로젝트 <br>Creating a translator using Transformers as a project|
|11|기계 번역이 걸어온 길 <br>The Journey of Machine Translation | 자연어 처리에서 Data Augmentation은 어떻게 하는지, 자연어 처리 성능은 어떻게 측정할 수 있는지 학습 <br>Learning how Data Augmentation is done in natural language processing and how the performance of natural language processing is measured|
|12|번역가는 대화에도 능하다 <br>Translators Are Proficient in Conversations Too | 다양한 디코딩 방식을 활용해 모델 구현 후 BLEU Score를 이용하여 성능 평가, 한국어 챗봇 구현 프로젝트 수행 <br>Implementing a model using various decoding methods, evaluating performance using BLEU Score, and performing a Korean chatbot implementation project|
|13|modern NLP의 흐름에 올라타보자 <br>Riding the Flow of Modern NLP | 트랜스포머를 바탕으로 한 최근 NLP 모델에 대해 학습 <br>Learning about recent NLP models based on Transformers|
|14|BERT pretrained model 제작 <br>Creating a BERT Pretrained Model | 대표적인 pretrained language model인 BERT 원리에 대해 학습 <br>Learning about the principles of BERT, a representative pretrained language model|
|15|NLP Framework의 활용 <br>Utilizing NLP Frameworks | 최다양한 NLP Framework에 대해 학습하고, Huggingface transformer를 중심으로 설계구조와 활용법 학습 <br>Learning about various NLP frameworks, focusing on the design structure and usage of Hugging Face Transformer|
|16|HuggingFace 커스텀 프로젝트 만들기 <br>Creating a Custom Project with Hugging Face | Huggingface transformer를 활용한 커스텀 프로젝트 수행 <br>Performing a custom project using Hugging Face Transformer|
|17|#NLP Trend Note 1 | 최신 LLM 소개, InstructGPT의 SFT, RM, PPO 학습 메커니즘 소개 <br>ntroduction to the latest LLM, introduction to the training mechanisms of InstructGPT's SFT, RM, PPO|
|18|#NLP Trend Note 2 <br> | KochatGPT 구현 <br>Implementation of KochatGPT|
