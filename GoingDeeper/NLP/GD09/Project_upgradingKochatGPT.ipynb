{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdc9db49",
   "metadata": {},
   "source": [
    "<b> Rubric 평가 </b>\n",
    "\n",
    "|평가문항|상세기준|\n",
    "|:------|:---|\n",
    "|1. 기존 데이터셋을 추가 정제하고, generation 성능을 끌어올리기 위한 기법들을 실험해 모델 perfomance를 향상시켜보았는가? |기존 데이터셋의 문제점을 분석하고 전처리 전략을 수립해 추가 정제를 진행했다. Beam search, Top-k(p) sampling 등 최선의 디코딩 전략을 수립해 향상된 모델 추론 결과를 제시했다. BLEU, ROUGE 등 생성된 텍스트를 평가하기 위한 메트릭을 적용한 정량적인 평가 결과와 주관적인 평가를 비교분석하였다.|\n",
    "|2. 새로운 데이터를 수집해 전처리를 수행하여 모델을 재학습시켜보았는가?|모두의 말뭉치, AI hub 등에 공개된 데이터를 사용해 추가 데이터셋을 구축하기 위한 기준과 근거를 수립했다. ChatGPT API나 다양한 한국어 benchmark 데이터셋을 활용해 Human Feedback 을 대체할 수 있는 아이디어를 구현했다. 위를 바탕으로 SFT, RM, PPO 세 단계에 필요한 각 데이터셋을 적절히 구축하여, 모델 추론 결과와 수립한 가설을 비교해보았다.|\n",
    "|3. 학습 전략 또는 foundation model을 변경해 모델을 재학습시켜보았는가?| 더 적절한 Instruction Tuning 기법을 적용해 SFT를 해보거나, Reward Model의 ranking algorithm을 개선해보았다. KoGPT-2가 아닌 다른 모델을 initial model로 사용하여 모델 학습을 성공시켰다. 허깅페이스의 accelerate, bitsandbytes 라이브러리 등을 사용하여 더 큰 스케일의 모델로 ChatGPT를 re-building해 모델 성능을 향상시켰다\n",
    "\n",
    "\n",
    "#### 1. 기존 데이터셋 추가 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a251afb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q nltk rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6547b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "import json \n",
    "from dataclasses import dataclass\n",
    "\n",
    "from typing import Optional, Dict, Sequence\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9176aae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d3bfc2956a47a79ee9d4efb5cbeac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65acbfdfebc4093a96054b7f054854a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bf2f9e4f414caa9fa21d60bb46e56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ac44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "        \n",
    "        sources = []\n",
    "        targets = []\n",
    "        \n",
    "        ###############################################\n",
    "        \n",
    "        data_path_1_SFT = '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl'\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict_1 = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT_1 = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input_1 = PROMPT_DICT_1[\"prompt_input\"]\n",
    "\n",
    "        for example in list_data_dict_1:\n",
    "            tmp = prompt_input_1.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        for example in list_data_dict_1:\n",
    "            targets.append(f\"{example['completion']}{tokenizer.eos_token}\")\n",
    "        \n",
    "        ###############################################\n",
    "        \n",
    "        data_path_2_SFT = './data/KoAlpaca_v1.1.jsonl'\n",
    "        list_data_dict_2 = []\n",
    "        with open(data_path_2_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            for line in json_file:\n",
    "                list_data_dict_2.append(json.loads(line))\n",
    "                \n",
    "        PROMPT_DICT_2 = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{instruction}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input_2 = PROMPT_DICT_2[\"prompt_input\"]\n",
    "        \n",
    "        for example in list_data_dict_2:\n",
    "            tmp = prompt_input_2.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        for example in list_data_dict_2:\n",
    "            targets.append(f\"{example['output']}{tokenizer.eos_token}\")\n",
    "        \n",
    "        ###############################################\n",
    "        \n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "        \n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d4f6bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "        data_path_1_SFT = '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl'\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0258596",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object): \n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21037e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([  739,   378,   378,   378, 14659, 13394, 37091, 10651,   383, 25841,\n",
      "         8006, 14914,   375,  7673, 20479,  8091, 22311,  9036, 30902, 13675,\n",
      "          375,   378,   378,   378, 41951,   454,  9549, 20549,   383,  8142,\n",
      "         7192, 14914,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n",
      "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SFT_dataset(data_path_1_SFT='./aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02e24ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"aiffel/KoChatGPT/test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df209f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 06:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.984100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.776800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.687200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained('/aiffel/KoChatGPT/output_1_SFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6648dcd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이기 때문에 불고기용 고기의 종류와 양에 대한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기는 쇠고기와 함께 먹는 음식 중 하나입니다. 따라서 불고기를 먹을 수 있는 종류는 다양합니다. 예를 들어, 닭가슴살 스테이크, 오므라이스 샐러드 등이 있습니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 42대 부통령직을 수행했습니다.作)作)은 \"리처드 닉슨\"이 41대 부통령을 수행한 년도를 가리키는 말입니다.作)는 \"리처드 닉슨\"이 40대 부통령을 맡았던 년도를 의미합니다.作은 \"리처드슨\"이 50대 부통령\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어?\n",
      "\n",
      "### Response(응답):'시카고 오 헤어 국제공항은 미국 캘리포니아주 샌프란시스코에 위치해 있습니다.子供共和國際空港)이라고 불립니다.子供公和国際空港이라는 뜻입니다.子供空和國際公港이라는 이름을 가진 항공사는 다음과 같습니다.\\n\\n1. 대한항공\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇으로써 미세먼지 정보를 알 수 없습니다. 미세먼지 예보를 확인해 보시는 것이 좋겠습니다.\\n\\n미세먼지 예보: 일반적으로 미세먼지는 주로 중국에서 발원하여 중국 전역으로 퍼져나가기 때문에 중국발 미세먼지가 유입될\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='/aiffel/KoChatGPT/output_1_SFT', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(   \n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n   \n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)   \n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba708e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_label = [\n",
    "    '저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.',\n",
    "    '1953년입니다.',\n",
    "    '시카고 오헤어 국제공항은 미국 일리노이 주 시카고에 위치해 있습니다.',\n",
    "    '미세먼지 농도는 어제와 비교해서 개선되었지만 아직도 나쁜 수준이며, 마스크 착용과 실외 활동 자제를 권장합니다. 정확한 미세먼지 농도를 확인하려면 해당 지역의 미세먼지 측정소에서 확인해보시기 바랍니다.',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c78fe69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q. 불고기용 고기 한우에요?\n",
      "\n",
      "G. '저는 인공지능 어시스턴트이기 때문에 불고기용 고기의 종류와 양에 대한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기는 쇠고기와 함께 먹는 음식 중 하나입니다. 따라서 불고기를 먹을 수 있는 종류는 다양합니다. 예를 들어, 닭가슴살 스테이크, 오므라이스 샐러드 등이 있습니다.\n",
      "\n",
      "A. 저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\n",
      "\n",
      "1-Gram BLEU : 0.02702702702702703\n",
      "2-Gram BLEU : 2.2250738585072626e-308\n",
      "3-Gram BLEU : 2.2250738585072626e-308\n",
      "4-Gram BLEU : 2.2250738585072626e-308\n",
      "--------------------------------------------------\n",
      "Q. 리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "G. '리처드 닉슨은 42대 부통령직을 수행했습니다.作)作)은 \"리처드 닉슨\"이 41대 부통령을 수행한 년도를 가리키는 말입니다.作)는 \"리처드 닉슨\"이 40대 부통령을 맡았던 년도를 의미합니다.作은 \"리처드슨\"이 50대 부통령\n",
      "\n",
      "A. 1953년입니다.\n",
      "\n",
      "1-Gram BLEU : 0\n",
      "2-Gram BLEU : 0\n",
      "3-Gram BLEU : 0\n",
      "4-Gram BLEU : 0\n",
      "--------------------------------------------------\n",
      "Q. 시카고 오헤어 국제공항은 어디에 있어?\n",
      "\n",
      "G. '시카고 오 헤어 국제공항은 미국 캘리포니아주 샌프란시스코에 위치해 있습니다.子供共和國際空港)이라고 불립니다.子供公和国際空港이라는 뜻입니다.子供空和國際公港이라는 이름을 가진 항공사는 다음과 같습니다.\\n\\n1. 대한항공\n",
      "\n",
      "A. 시카고 오헤어 국제공항은 미국 일리노이 주 시카고에 위치해 있습니다.\n",
      "\n",
      "1-Gram BLEU : 0.0588235294117647\n",
      "2-Gram BLEU : 2.2250738585072626e-308\n",
      "3-Gram BLEU : 2.2250738585072626e-308\n",
      "4-Gram BLEU : 2.2250738585072626e-308\n",
      "--------------------------------------------------\n",
      "Q. 오늘 미세먼지 어때?\n",
      "\n",
      "G. '저는 인공지능 챗봇으로써 미세먼지 정보를 알 수 없습니다. 미세먼지 예보를 확인해 보시는 것이 좋겠습니다.\\n\\n미세먼지 예보: 일반적으로 미세먼지는 주로 중국에서 발원하여 중국 전역으로 퍼져나가기 때문에 중국발 미세먼지가 유입될\n",
      "\n",
      "A. 미세먼지 농도는 어제와 비교해서 개선되었지만 아직도 나쁜 수준이며, 마스크 착용과 실외 활동 자제를 권장합니다. 정확한 미세먼지 농도를 확인하려면 해당 지역의 미세먼지 측정소에서 확인해보시기 바랍니다.\n",
      "\n",
      "1-Gram BLEU : 0.037037037037037035\n",
      "2-Gram BLEU : 2.2250738585072626e-308\n",
      "3-Gram BLEU : 2.2250738585072626e-308\n",
      "4-Gram BLEU : 2.2250738585072626e-308\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "for prompt, result, label in zip(list_prompt, list_result, list_data_label):\n",
    "    q = prompt[prompt.index('\\n')+1:prompt.rindex('\\n')]\n",
    "    print('Q.', q )\n",
    "    g = result[0]['generated_text'][result[0]['generated_text'].rindex('(응답):')+5:]\n",
    "    print('G.', g )\n",
    "    print()\n",
    "    a = label # 정답\n",
    "    print('A.', label)\n",
    "    print()\n",
    "    print(\"1-Gram BLEU :\", sentence_bleu(a.split(), g.split(), weights=(1, 0, 0, 0 )))  \n",
    "    print(\"2-Gram BLEU :\", sentence_bleu(a.split(), g.split(), weights=(0, 1, 0 ,0 )))  \n",
    "    print(\"3-Gram BLEU :\", sentence_bleu(a.split(), g.split(), weights=(0, 0, 1 ,0 )))  \n",
    "    print(\"4-Gram BLEU :\", sentence_bleu(a.split(), g.split(), weights=(0, 0, 0, 1 ))) \n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e47e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c65e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644da3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8620af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66a88f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff75cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ffd879f",
   "metadata": {},
   "source": [
    "#### RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28035f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chatgpt in /opt/conda/lib/python3.9/site-packages (0.1.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (from chatgpt) (2.13.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from chatgpt) (1.12.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from chatgpt) (4.62.3)\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.9/site-packages (from chatgpt) (0.0.113)\n",
      "Requirement already satisfied: transformers>=4.20.1 in /opt/conda/lib/python3.9/site-packages (from chatgpt) (4.28.0)\n",
      "Requirement already satisfied: colossalai>=0.2.4 in /opt/conda/lib/python3.9/site-packages (from chatgpt) (0.2.7)\n",
      "Requirement already satisfied: loralib in /opt/conda/lib/python3.9/site-packages (from chatgpt) (0.1.1)\n",
      "Requirement already satisfied: pre-commit in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt) (3.3.3)\n",
      "Requirement already satisfied: fabric in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt) (3.1.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt) (5.8.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt) (21.3)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt) (1.11.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt) (1.21.4)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt) (8.0.3)\n",
      "Requirement already satisfied: contexttimer in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt) (0.3.3)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt) (13.4.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt) (6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt) (3.12.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt) (2021.11.10)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt) (0.15.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt) (0.13.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt) (2.0.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt) (2021.11.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt) (12.0.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt) (1.3.3)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt) (3.8.4)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt) (0.3.4)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt) (0.70.12.2)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in /opt/conda/lib/python3.9/site-packages (from langchain->chatgpt) (1.4.48)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /opt/conda/lib/python3.9/site-packages (from langchain->chatgpt) (1.10.9)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/conda/lib/python3.9/site-packages (from langchain->chatgpt) (0.5.8)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.9/site-packages (from langchain->chatgpt) (8.2.2)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt) (4.7.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt) (4.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt) (1.7.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt) (2.0.8)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt) (5.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt) (21.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt) (0.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->colossalai>=0.2.4->chatgpt) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.20.1->chatgpt) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.20.1->chatgpt) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.20.1->chatgpt) (1.26.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.9/site-packages (from SQLAlchemy<2,>=1->langchain->chatgpt) (2.0.2)\n",
      "Requirement already satisfied: invoke>=2.0 in /opt/conda/lib/python3.9/site-packages (from fabric->colossalai>=0.2.4->chatgpt) (2.1.3)\n",
      "Requirement already satisfied: decorator>=5 in /opt/conda/lib/python3.9/site-packages (from fabric->colossalai>=0.2.4->chatgpt) (5.1.1)\n",
      "Requirement already satisfied: paramiko>=2.4 in /opt/conda/lib/python3.9/site-packages (from fabric->colossalai>=0.2.4->chatgpt) (3.2.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->chatgpt) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->chatgpt) (2.8.2)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt) (3.3.1)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt) (1.8.0)\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt) (2.5.24)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt) (20.23.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from rich->colossalai>=0.2.4->chatgpt) (2.15.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.9/site-packages (from rich->colossalai>=0.2.4->chatgpt) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->colossalai>=0.2.4->chatgpt) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from nodeenv>=0.11.1->pre-commit->colossalai>=0.2.4->chatgpt) (59.4.0)\n",
      "Requirement already satisfied: bcrypt>=3.2 in /opt/conda/lib/python3.9/site-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt) (4.0.1)\n",
      "Requirement already satisfied: pynacl>=1.5 in /opt/conda/lib/python3.9/site-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt) (1.5.0)\n",
      "Requirement already satisfied: cryptography>=3.3 in /opt/conda/lib/python3.9/site-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt) (41.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets->chatgpt) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt) (0.4.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: distlib<1,>=0.3.6 in /opt/conda/lib/python3.9/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt) (0.3.6)\n",
      "Requirement already satisfied: platformdirs<4,>=3.5.1 in /opt/conda/lib/python3.9/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt) (3.8.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.9/site-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt) (2.21)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1178a952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /aiffel/aiffel/KoChatGPT/colossalai_ChatGPT_230319\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers>=4.20.1 in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (4.28.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (4.62.3)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (2.13.1)\n",
      "Requirement already satisfied: loralib in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (0.1.1)\n",
      "Requirement already satisfied: colossalai>=0.2.4 in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (0.2.7)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (1.12.1)\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.9/site-packages (from chatgpt==0.1.0) (0.0.113)\n",
      "Requirement already satisfied: contexttimer in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (0.3.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.21.4)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (8.0.3)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.11.1)\n",
      "Requirement already satisfied: fabric in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (21.3)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (13.4.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (5.8.0)\n",
      "Requirement already satisfied: pre-commit in /opt/conda/lib/python3.9/site-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.3.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (3.12.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2.26.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2021.11.10)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.15.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (2021.11.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (0.70.12.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (12.0.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (3.8.4)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (0.3.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (2.0.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets->chatgpt==0.1.0) (1.3.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.9/site-packages (from langchain->chatgpt==0.1.0) (8.2.2)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /opt/conda/lib/python3.9/site-packages (from langchain->chatgpt==0.1.0) (1.10.9)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in /opt/conda/lib/python3.9/site-packages (from langchain->chatgpt==0.1.0) (1.4.48)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/conda/lib/python3.9/site-packages (from langchain->chatgpt==0.1.0) (0.5.8)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.9/site-packages (from torch->chatgpt==0.1.0) (4.7.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.7.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (2.0.8)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (4.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (21.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets->chatgpt==0.1.0) (5.2.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (0.9.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (1.5.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (3.19.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->colossalai>=0.2.4->chatgpt==0.1.0) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2.10)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.9/site-packages (from SQLAlchemy<2,>=1->langchain->chatgpt==0.1.0) (2.0.2)\n",
      "Requirement already satisfied: paramiko>=2.4 in /opt/conda/lib/python3.9/site-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (3.2.0)\n",
      "Requirement already satisfied: invoke>=2.0 in /opt/conda/lib/python3.9/site-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.1.3)\n",
      "Requirement already satisfied: decorator>=5 in /opt/conda/lib/python3.9/site-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (5.1.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->chatgpt==0.1.0) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets->chatgpt==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (20.23.1)\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (2.5.24)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.3.1)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (1.8.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.9/site-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.15.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->colossalai>=0.2.4->chatgpt==0.1.0) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from nodeenv>=0.11.1->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (59.4.0)\n",
      "Requirement already satisfied: cryptography>=3.3 in /opt/conda/lib/python3.9/site-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (41.0.1)\n",
      "Requirement already satisfied: bcrypt>=3.2 in /opt/conda/lib/python3.9/site-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (4.0.1)\n",
      "Requirement already satisfied: pynacl>=1.5 in /opt/conda/lib/python3.9/site-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets->chatgpt==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (0.4.3)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /opt/conda/lib/python3.9/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (0.3.6)\n",
      "Requirement already satisfied: platformdirs<4,>=3.5.1 in /opt/conda/lib/python3.9/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.8.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.9/site-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.21)\n",
      "Building wheels for collected packages: chatgpt\n",
      "  Building wheel for chatgpt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for chatgpt: filename=chatgpt-0.1.0-py3-none-any.whl size=46664 sha256=1ad7537047aa7449add8f1b98ec1c51d631fd240dbd080813fd36e96e9884e7b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-48yrm_98/wheels/79/25/c3/338e0c56a2253a8ea6c41e8692f6eb2409a3898c63b234b103\n",
      "Successfully built chatgpt\n",
      "Installing collected packages: chatgpt\n",
      "  Attempting uninstall: chatgpt\n",
      "    Found existing installation: chatgpt 0.1.0\n",
      "    Uninstalling chatgpt-0.1.0:\n",
      "      Successfully uninstalled chatgpt-0.1.0\n",
      "Successfully installed chatgpt-0.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"$HOME/aiffel/KoChatGPT/colossalai_ChatGPT_230319/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db89ce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import json\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import loralib as lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87321c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "807fbdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f31ab094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data num: 10220\n",
      "after  data num: 30660\n",
      "data example: \n",
      "{'prompt': '애플은 리사를 어떻게 처리했어', 'chosen': '애플이 누구인지 명확히 알 수 없어서, 리사가 누구인지와 어떤 상황에서 처리되었는지에 대한 추가적인 정보가 필요합니다. 따라서, 보다 정확한 답변을 제공할 수 없습니다.', 'rejected': '애플은 리사를 위해 고객 서비스 부서에서 고객 다양한 컴퓨터 관련 문제에 대해 응답하는 데 필요한 모든 지원을 제공했습니다. 사용자가 하드웨어 문제를 경험할 때, 전문가들은 필요한 수리(수리, 추가 부품 제공, 소프트웨어 업그레이드 등)을 제공해 드릴 수 있습니다. 또한, 사용자가 사용 방법 문제나 기타 문제를 경험할 때, 대화 상대로 사용자를 지원할 수 있는 전문 고객 서비스 직원들이 사용자에게 상담하고 도움을 주는 데 도움이 될 수 있는 정보를 제공합니다. 또한, 인터넷에서 제공되는 정보를 통해 문제를 해결하거나 고객 서비스 웹 사이트를 통해 자신의 문제를 진단할 수 있도록 하는 등 다양한 방법으로 리사를 처리해 왔습니다.'}\n"
     ]
    }
   ],
   "source": [
    "with open('/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97af4e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은?', 'chosen': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은 류승완의 사무실입니다.', 'rejected': '대구 영화사옥'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(230319)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02c8ba63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1276.82it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1262.73it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = total_data_ranking2chosen[:1000] \n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1067d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "## prompt ##\n",
      "흑고래의 무게는 어느 정도야\n",
      "######################################################################\n",
      "## chosen ##\n",
      "흑고래의 평균 몸무게는 약 25~40톤 정도이지만, 최대 몸무게는 50톤 이상에 이를 수 있습니다.\n",
      "######################################################################\n",
      "## rejected ##\n",
      "흑고래의 무게는 매우 다양하게 달라집니다. 약 200kg에서 10톤까지 달라질 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "015c056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=Adam(model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=4,\n",
    "                             max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f605f00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Train step of epoch 0:   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:00<03:44,  1.11it/s]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:00<03:44,  1.11it/s, loss=0.721]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:01<03:37,  1.14it/s, loss=0.721]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:01<03:37,  1.14it/s, loss=0.751]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:34,  1.15it/s, loss=0.751]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:34,  1.15it/s, loss=0.534]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:33,  1.15it/s, loss=0.534]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:33,  1.15it/s, loss=0.476]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:31,  1.16it/s, loss=0.476]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:31,  1.16it/s, loss=0.448]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:31,  1.16it/s, loss=0.448]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:31,  1.16it/s, loss=1.22] \u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:30,  1.16it/s, loss=1.22]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:30,  1.16it/s, loss=0.467]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:06<03:29,  1.16it/s, loss=0.467]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:06<03:29,  1.16it/s, loss=1.44] \u001b[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:07<03:28,  1.16it/s, loss=1.44]\u001b[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:07<03:28,  1.16it/s, loss=0.419]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:08<03:27,  1.15it/s, loss=0.419]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:08<03:27,  1.15it/s, loss=0.508]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:27,  1.15it/s, loss=0.508]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:27,  1.15it/s, loss=0.485]\u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:26,  1.15it/s, loss=0.485]\u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:26,  1.15it/s, loss=0.399]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:25,  1.15it/s, loss=0.399]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:25,  1.15it/s, loss=0.153]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:24,  1.15it/s, loss=0.153]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:24,  1.15it/s, loss=1.1]  \u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:13<03:24,  1.15it/s, loss=1.1]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:13<03:24,  1.15it/s, loss=0.567]\u001b[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:13<03:23,  1.15it/s, loss=0.567]\u001b[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:13<03:23,  1.15it/s, loss=1.34] \u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:14<03:22,  1.15it/s, loss=1.34]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:14<03:22,  1.15it/s, loss=0.703]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:15<03:22,  1.15it/s, loss=0.703]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:15<03:22,  1.15it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:16<03:21,  1.15it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:16<03:21,  1.15it/s, loss=0.662]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:17<03:20,  1.15it/s, loss=0.662]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:17<03:20,  1.15it/s, loss=0.625]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:18<03:19,  1.15it/s, loss=0.625]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:18<03:19,  1.15it/s, loss=0.653]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:19<03:19,  1.14it/s, loss=0.653]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:19<03:19,  1.14it/s, loss=0.679]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:20<03:18,  1.14it/s, loss=0.679]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:20<03:18,  1.14it/s, loss=0.798]\u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:20<03:17,  1.14it/s, loss=0.798]\u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:20<03:17,  1.14it/s, loss=0.622]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:21<03:17,  1.14it/s, loss=0.622]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:21<03:17,  1.14it/s, loss=0.623]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:22<03:16,  1.14it/s, loss=0.623]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:22<03:16,  1.14it/s, loss=0.721]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:23<03:15,  1.14it/s, loss=0.721]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:23<03:15,  1.14it/s, loss=0.615]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:24<03:15,  1.14it/s, loss=0.615]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:24<03:15,  1.14it/s, loss=0.576]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:25<03:14,  1.14it/s, loss=0.576]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:25<03:14,  1.14it/s, loss=0.482]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:26<03:13,  1.14it/s, loss=0.482]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:26<03:13,  1.14it/s, loss=0.578]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:27<03:12,  1.14it/s, loss=0.578]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:27<03:12,  1.14it/s, loss=0.467]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:27<03:12,  1.13it/s, loss=0.467]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:27<03:12,  1.13it/s, loss=0.865]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:28<03:11,  1.13it/s, loss=0.865]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:28<03:11,  1.13it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:29<03:10,  1.13it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:29<03:10,  1.13it/s, loss=0.452]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:30<03:10,  1.13it/s, loss=0.452]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:30<03:10,  1.13it/s, loss=0.824]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:31<03:09,  1.13it/s, loss=0.824]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:31<03:09,  1.13it/s, loss=1.11] \u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:32<03:08,  1.13it/s, loss=1.11]\u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:32<03:08,  1.13it/s, loss=0.848]\u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:33<03:07,  1.13it/s, loss=0.848]\u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:33<03:07,  1.13it/s, loss=0.458]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:34<03:07,  1.13it/s, loss=0.458]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:34<03:07,  1.13it/s, loss=0.725]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:35<03:06,  1.13it/s, loss=0.725]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:35<03:06,  1.13it/s, loss=0.395]\u001b[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:35<03:05,  1.13it/s, loss=0.395]\u001b[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:35<03:05,  1.13it/s, loss=0.423]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:36<03:04,  1.13it/s, loss=0.423]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:36<03:04,  1.13it/s, loss=0.646]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:37<03:04,  1.12it/s, loss=0.646]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:37<03:04,  1.12it/s, loss=0.781]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:38<03:03,  1.12it/s, loss=0.781]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:38<03:03,  1.12it/s, loss=0.987]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:39<03:02,  1.12it/s, loss=0.987]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:39<03:02,  1.12it/s, loss=0.61] \u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:40<03:01,  1.12it/s, loss=0.61]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:40<03:01,  1.12it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:41<03:01,  1.12it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:41<03:01,  1.12it/s, loss=0.672]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:42<03:00,  1.12it/s, loss=0.672]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:42<03:00,  1.12it/s, loss=0.669]\u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:43<02:59,  1.12it/s, loss=0.669]\u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:43<02:59,  1.12it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:43<02:58,  1.12it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:43<02:58,  1.12it/s, loss=0.847]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:44<02:57,  1.12it/s, loss=0.847]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:44<02:57,  1.12it/s, loss=0.457]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:45<02:57,  1.12it/s, loss=0.457]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:45<02:57,  1.12it/s, loss=0.571]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:46<02:56,  1.12it/s, loss=0.571]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:46<02:56,  1.12it/s, loss=0.411]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:47<02:55,  1.12it/s, loss=0.411]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:47<02:55,  1.12it/s, loss=0.333]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:48<02:55,  1.11it/s, loss=0.333]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:48<02:55,  1.11it/s, loss=0.568]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:49<02:54,  1.11it/s, loss=0.568]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:49<02:54,  1.11it/s, loss=0.563]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:50<02:53,  1.11it/s, loss=0.563]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:50<02:53,  1.11it/s, loss=0.539]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:51<02:52,  1.11it/s, loss=0.539]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:51<02:52,  1.11it/s, loss=0.584]\u001b[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:52<02:51,  1.11it/s, loss=0.584]\u001b[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:52<02:51,  1.11it/s, loss=0.652]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:52<02:50,  1.11it/s, loss=0.652]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:52<02:50,  1.11it/s, loss=0.326]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:53<02:49,  1.11it/s, loss=0.326]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:53<02:49,  1.11it/s, loss=1.22] \u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:54<02:49,  1.11it/s, loss=1.22]\u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:54<02:49,  1.11it/s, loss=0.175]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:55<02:48,  1.11it/s, loss=0.175]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:55<02:48,  1.11it/s, loss=0.858]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:56<02:47,  1.11it/s, loss=0.858]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:56<02:47,  1.11it/s, loss=0.346]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:57<02:46,  1.11it/s, loss=0.346]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:57<02:46,  1.11it/s, loss=0.767]\u001b[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [00:58<02:45,  1.11it/s, loss=0.767]\u001b[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [00:58<02:45,  1.11it/s, loss=0.188]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [00:59<02:44,  1.11it/s, loss=0.188]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [00:59<02:44,  1.11it/s, loss=0.861]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [01:00<02:43,  1.11it/s, loss=0.861]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [01:00<02:43,  1.11it/s, loss=0.38] \u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [01:01<02:42,  1.11it/s, loss=0.38]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [01:01<02:42,  1.11it/s, loss=0.791]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:01<02:41,  1.11it/s, loss=0.791]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:01<02:41,  1.11it/s, loss=0.731]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:02<02:40,  1.11it/s, loss=0.731]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:02<02:40,  1.11it/s, loss=0.528]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:03<02:39,  1.11it/s, loss=0.528]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:03<02:39,  1.11it/s, loss=0.775]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:04<02:39,  1.11it/s, loss=0.775]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:04<02:39,  1.11it/s, loss=0.714]\u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:05<02:37,  1.11it/s, loss=0.714]\u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:05<02:37,  1.11it/s, loss=0.591]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:06<02:36,  1.11it/s, loss=0.591]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:06<02:36,  1.11it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:07<02:35,  1.12it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:07<02:35,  1.12it/s, loss=0.723]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:08<02:34,  1.12it/s, loss=0.723]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:08<02:34,  1.12it/s, loss=0.444]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:09<02:34,  1.12it/s, loss=0.444]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:09<02:34,  1.12it/s, loss=0.628]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:09<02:32,  1.12it/s, loss=0.628]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:10<02:32,  1.12it/s, loss=0.549]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:10<02:31,  1.12it/s, loss=0.549]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:10<02:31,  1.12it/s, loss=0.689]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:11<02:31,  1.12it/s, loss=0.689]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:11<02:31,  1.12it/s, loss=0.408]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:12<02:30,  1.12it/s, loss=0.408]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:12<02:30,  1.12it/s, loss=0.833]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:13<02:29,  1.12it/s, loss=0.833]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:13<02:29,  1.12it/s, loss=0.485]\u001b[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:14<02:28,  1.12it/s, loss=0.485]\u001b[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:14<02:28,  1.12it/s, loss=0.637]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:15<02:27,  1.12it/s, loss=0.637]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:15<02:27,  1.12it/s, loss=0.727]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:16<02:26,  1.12it/s, loss=0.727]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:16<02:26,  1.12it/s, loss=0.622]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:17<02:25,  1.12it/s, loss=0.622]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:17<02:25,  1.12it/s, loss=0.465]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:18<02:24,  1.12it/s, loss=0.465]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:18<02:24,  1.12it/s, loss=0.919]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:18<02:23,  1.12it/s, loss=0.919]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:18<02:23,  1.12it/s, loss=0.821]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:19<02:22,  1.12it/s, loss=0.821]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:19<02:22,  1.12it/s, loss=0.757]\u001b[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:20<02:21,  1.12it/s, loss=0.757]\u001b[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:20<02:21,  1.12it/s, loss=0.61] \u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:21<02:20,  1.12it/s, loss=0.61]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:21<02:20,  1.12it/s, loss=0.595]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:22<02:19,  1.12it/s, loss=0.595]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:22<02:19,  1.12it/s, loss=0.856]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:23<02:18,  1.12it/s, loss=0.856]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:23<02:18,  1.12it/s, loss=0.725]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:24<02:17,  1.13it/s, loss=0.725]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:24<02:17,  1.13it/s, loss=0.495]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:25<02:16,  1.13it/s, loss=0.495]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:25<02:16,  1.13it/s, loss=0.703]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:26<02:15,  1.13it/s, loss=0.703]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:26<02:15,  1.13it/s, loss=0.811]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:26<02:14,  1.13it/s, loss=0.811]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:26<02:14,  1.13it/s, loss=0.756]\u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:27<02:14,  1.13it/s, loss=0.756]\u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:27<02:14,  1.13it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:28<02:13,  1.13it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:28<02:13,  1.13it/s, loss=0.657]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:29<02:12,  1.13it/s, loss=0.657]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:29<02:12,  1.13it/s, loss=0.7]  \u001b[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:30<02:11,  1.13it/s, loss=0.7]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:30<02:11,  1.13it/s, loss=0.684]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:31<02:10,  1.13it/s, loss=0.684]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:31<02:10,  1.13it/s, loss=0.734]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:32<02:09,  1.13it/s, loss=0.734]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:32<02:09,  1.13it/s, loss=0.765]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:33<02:08,  1.13it/s, loss=0.765]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:33<02:08,  1.13it/s, loss=0.65] \u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:33<02:07,  1.13it/s, loss=0.65]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:34<02:07,  1.13it/s, loss=0.682]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:34<02:06,  1.13it/s, loss=0.682]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:34<02:06,  1.13it/s, loss=0.725]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:35<02:05,  1.13it/s, loss=0.725]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:35<02:05,  1.13it/s, loss=0.606]\u001b[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:36<02:04,  1.13it/s, loss=0.606]\u001b[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:36<02:04,  1.13it/s, loss=0.66] \u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:37<02:03,  1.13it/s, loss=0.66]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:37<02:03,  1.13it/s, loss=0.757]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:38<02:02,  1.13it/s, loss=0.757]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:38<02:02,  1.13it/s, loss=0.605]\u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:39<02:02,  1.13it/s, loss=0.605]\u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:39<02:02,  1.13it/s, loss=0.656]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:40<02:01,  1.13it/s, loss=0.656]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:40<02:01,  1.13it/s, loss=0.656]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:41<02:00,  1.13it/s, loss=0.656]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:41<02:00,  1.13it/s, loss=0.829]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:41<01:59,  1.13it/s, loss=0.829]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:41<01:59,  1.13it/s, loss=0.668]\u001b[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:42<01:58,  1.13it/s, loss=0.668]\u001b[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:42<01:58,  1.13it/s, loss=0.588]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:43<01:57,  1.13it/s, loss=0.588]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:43<01:57,  1.13it/s, loss=0.558]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:44<01:56,  1.13it/s, loss=0.558]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:44<01:56,  1.13it/s, loss=0.502]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:45<01:55,  1.13it/s, loss=0.502]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:45<01:55,  1.13it/s, loss=0.522]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:46<01:54,  1.13it/s, loss=0.522]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:46<01:54,  1.13it/s, loss=0.997]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:47<01:53,  1.13it/s, loss=0.997]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:47<01:53,  1.13it/s, loss=0.786]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:48<01:52,  1.13it/s, loss=0.786]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:48<01:52,  1.13it/s, loss=0.662]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:49<01:52,  1.13it/s, loss=0.662]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:49<01:52,  1.13it/s, loss=0.549]\u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:49<01:51,  1.13it/s, loss=0.549]\u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:49<01:51,  1.13it/s, loss=0.747]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:50<01:50,  1.13it/s, loss=0.747]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:50<01:50,  1.13it/s, loss=0.663]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:51<01:49,  1.13it/s, loss=0.663]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:51<01:49,  1.13it/s, loss=0.698]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:52<01:48,  1.13it/s, loss=0.698]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:52<01:48,  1.13it/s, loss=0.568]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:53<01:47,  1.13it/s, loss=0.568]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:53<01:47,  1.13it/s, loss=0.627]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:54<01:46,  1.13it/s, loss=0.627]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:54<01:46,  1.13it/s, loss=0.641]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:55<01:46,  1.13it/s, loss=0.641]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:55<01:46,  1.13it/s, loss=0.571]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:56<01:45,  1.13it/s, loss=0.571]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:56<01:45,  1.13it/s, loss=0.909]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:56<01:44,  1.13it/s, loss=0.909]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:56<01:44,  1.13it/s, loss=0.597]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:57<01:43,  1.13it/s, loss=0.597]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:57<01:43,  1.13it/s, loss=0.784]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [01:58<01:42,  1.13it/s, loss=0.784]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [01:58<01:42,  1.13it/s, loss=0.73] \u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [01:59<01:41,  1.13it/s, loss=0.73]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [01:59<01:41,  1.13it/s, loss=0.669]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [02:00<01:40,  1.13it/s, loss=0.669]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [02:00<01:40,  1.13it/s, loss=0.679]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [02:01<01:39,  1.13it/s, loss=0.679]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [02:01<01:39,  1.13it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [02:02<01:39,  1.13it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [02:02<01:39,  1.13it/s, loss=0.653]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:03<01:38,  1.13it/s, loss=0.653]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:03<01:38,  1.13it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:04<01:37,  1.13it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:04<01:37,  1.13it/s, loss=0.494]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:04<01:36,  1.13it/s, loss=0.494]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:04<01:36,  1.13it/s, loss=0.622]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:05<01:35,  1.13it/s, loss=0.622]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:05<01:35,  1.13it/s, loss=0.867]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:06<01:34,  1.13it/s, loss=0.867]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:06<01:34,  1.13it/s, loss=0.551]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:07<01:33,  1.13it/s, loss=0.551]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:07<01:33,  1.13it/s, loss=0.513]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:08<01:32,  1.13it/s, loss=0.513]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:08<01:32,  1.13it/s, loss=0.655]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:09<01:32,  1.13it/s, loss=0.655]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:09<01:32,  1.13it/s, loss=0.594]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:10<01:31,  1.13it/s, loss=0.594]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:10<01:31,  1.13it/s, loss=0.383]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:11<01:30,  1.13it/s, loss=0.383]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:11<01:30,  1.13it/s, loss=0.614]\u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:11<01:29,  1.13it/s, loss=0.614]\u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:12<01:29,  1.13it/s, loss=0.667]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:12<01:28,  1.13it/s, loss=0.667]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:12<01:28,  1.13it/s, loss=0.607]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:13<01:27,  1.13it/s, loss=0.607]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:13<01:27,  1.13it/s, loss=0.222]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:14<01:26,  1.13it/s, loss=0.222]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:14<01:26,  1.13it/s, loss=0.517]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:15<01:26,  1.13it/s, loss=0.517]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:15<01:26,  1.13it/s, loss=0.596]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:16<01:25,  1.13it/s, loss=0.596]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:16<01:25,  1.13it/s, loss=1.11] \u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:17<01:24,  1.13it/s, loss=1.11]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:17<01:24,  1.13it/s, loss=1.16]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:18<01:23,  1.13it/s, loss=1.16]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:18<01:23,  1.13it/s, loss=0.415]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:19<01:22,  1.13it/s, loss=0.415]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:19<01:22,  1.13it/s, loss=0.483]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:19<01:21,  1.12it/s, loss=0.483]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:20<01:21,  1.12it/s, loss=0.694]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:20<01:20,  1.12it/s, loss=0.694]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:20<01:20,  1.12it/s, loss=0.762]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:21<01:20,  1.12it/s, loss=0.762]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:21<01:20,  1.12it/s, loss=0.426]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:22<01:19,  1.12it/s, loss=0.426]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:22<01:19,  1.12it/s, loss=0.669]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:23<01:18,  1.12it/s, loss=0.669]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:23<01:18,  1.12it/s, loss=0.5]  \u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:24<01:17,  1.12it/s, loss=0.5]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:24<01:17,  1.12it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:25<01:16,  1.12it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:25<01:16,  1.12it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:26<01:15,  1.12it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:26<01:15,  1.12it/s, loss=0.971]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:27<01:14,  1.12it/s, loss=0.971]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:27<01:14,  1.12it/s, loss=0.353]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:28<01:13,  1.12it/s, loss=0.353]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:28<01:13,  1.12it/s, loss=0.431]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:28<01:13,  1.12it/s, loss=0.431]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:28<01:13,  1.12it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:29<01:12,  1.12it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:29<01:12,  1.12it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:30<01:11,  1.12it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:30<01:11,  1.12it/s, loss=0.896]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:31<01:10,  1.12it/s, loss=0.896]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:31<01:10,  1.12it/s, loss=0.642]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:32<01:09,  1.12it/s, loss=0.642]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:32<01:09,  1.12it/s, loss=0.818]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:33<01:08,  1.12it/s, loss=0.818]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:33<01:08,  1.12it/s, loss=0.434]\u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:34<01:07,  1.12it/s, loss=0.434]\u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:34<01:07,  1.12it/s, loss=0.412]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:35<01:06,  1.12it/s, loss=0.412]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:35<01:06,  1.12it/s, loss=0.62] \u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:36<01:05,  1.12it/s, loss=0.62]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:36<01:05,  1.12it/s, loss=0.453]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:36<01:05,  1.12it/s, loss=0.453]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:36<01:05,  1.12it/s, loss=0.365]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:37<01:04,  1.12it/s, loss=0.365]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:37<01:04,  1.12it/s, loss=0.371]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:38<01:03,  1.12it/s, loss=0.371]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:38<01:03,  1.12it/s, loss=0.545]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:39<01:02,  1.12it/s, loss=0.545]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:39<01:02,  1.12it/s, loss=0.707]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:40<01:01,  1.12it/s, loss=0.707]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:40<01:01,  1.12it/s, loss=0.268]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:41<01:00,  1.12it/s, loss=0.268]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:41<01:00,  1.12it/s, loss=0.987]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:42<00:59,  1.12it/s, loss=0.987]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:42<00:59,  1.12it/s, loss=1.14] \u001b[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:43<00:58,  1.12it/s, loss=1.14]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:43<00:58,  1.12it/s, loss=1.02]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:44<00:57,  1.12it/s, loss=1.02]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:44<00:57,  1.12it/s, loss=0.559]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:44<00:56,  1.12it/s, loss=0.559]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:44<00:56,  1.12it/s, loss=0.378]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:45<00:56,  1.12it/s, loss=0.378]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:45<00:56,  1.12it/s, loss=0.378]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:46<00:55,  1.12it/s, loss=0.378]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:46<00:55,  1.12it/s, loss=0.648]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:47<00:54,  1.12it/s, loss=0.648]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:47<00:54,  1.12it/s, loss=0.869]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:48<00:53,  1.12it/s, loss=0.869]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:48<00:53,  1.12it/s, loss=0.398]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:49<00:52,  1.12it/s, loss=0.398]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:49<00:52,  1.12it/s, loss=0.28] \u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:50<00:51,  1.12it/s, loss=0.28]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:50<00:51,  1.12it/s, loss=0.469]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:51<00:50,  1.12it/s, loss=0.469]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:51<00:50,  1.12it/s, loss=0.858]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:52<00:49,  1.12it/s, loss=0.858]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:52<00:49,  1.12it/s, loss=0.694]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:52<00:48,  1.12it/s, loss=0.694]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:52<00:48,  1.12it/s, loss=0.443]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:53<00:48,  1.12it/s, loss=0.443]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:53<00:48,  1.12it/s, loss=1.02] \u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:54<00:47,  1.13it/s, loss=1.02]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:54<00:47,  1.13it/s, loss=0.675]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:55<00:46,  1.13it/s, loss=0.675]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:55<00:46,  1.13it/s, loss=0.557]\u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:56<00:45,  1.12it/s, loss=0.557]\u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:56<00:45,  1.12it/s, loss=0.559]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [02:57<00:44,  1.12it/s, loss=0.559]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [02:57<00:44,  1.12it/s, loss=0.47] \u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [02:58<00:43,  1.12it/s, loss=0.47]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [02:58<00:43,  1.12it/s, loss=0.486]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [02:59<00:42,  1.13it/s, loss=0.486]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [02:59<00:42,  1.13it/s, loss=0.771]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [03:00<00:41,  1.13it/s, loss=0.771]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [03:00<00:41,  1.13it/s, loss=0.637]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [03:00<00:40,  1.13it/s, loss=0.637]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [03:00<00:40,  1.13it/s, loss=0.495]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [03:01<00:39,  1.13it/s, loss=0.495]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [03:01<00:39,  1.13it/s, loss=0.562]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [03:02<00:39,  1.13it/s, loss=0.562]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [03:02<00:39,  1.13it/s, loss=0.772]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [03:03<00:38,  1.13it/s, loss=0.772]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [03:03<00:38,  1.13it/s, loss=0.55] \u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:04<00:37,  1.13it/s, loss=0.55]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:04<00:37,  1.13it/s, loss=0.72]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:05<00:36,  1.13it/s, loss=0.72]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:05<00:36,  1.13it/s, loss=0.688]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:06<00:35,  1.13it/s, loss=0.688]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:06<00:35,  1.13it/s, loss=0.585]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:07<00:34,  1.13it/s, loss=0.585]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:07<00:34,  1.13it/s, loss=0.408]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:08<00:33,  1.13it/s, loss=0.408]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:08<00:33,  1.13it/s, loss=0.574]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:08<00:32,  1.13it/s, loss=0.574]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:08<00:32,  1.13it/s, loss=0.375]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:09<00:31,  1.13it/s, loss=0.375]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:09<00:31,  1.13it/s, loss=1.1]  \u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:10<00:31,  1.13it/s, loss=1.1]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:10<00:31,  1.13it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:11<00:30,  1.13it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:11<00:30,  1.13it/s, loss=0.853]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:12<00:29,  1.13it/s, loss=0.853]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:12<00:29,  1.13it/s, loss=0.814]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:13<00:28,  1.13it/s, loss=0.814]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:13<00:28,  1.13it/s, loss=0.561]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:14<00:27,  1.13it/s, loss=0.561]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:14<00:27,  1.13it/s, loss=0.392]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:15<00:26,  1.13it/s, loss=0.392]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:15<00:26,  1.13it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:16<00:25,  1.13it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:16<00:25,  1.13it/s, loss=0.626]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:16<00:24,  1.13it/s, loss=0.626]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:16<00:24,  1.13it/s, loss=0.331]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:17<00:23,  1.13it/s, loss=0.331]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:17<00:23,  1.13it/s, loss=0.756]\u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:18<00:23,  1.13it/s, loss=0.756]\u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:18<00:23,  1.13it/s, loss=0.764]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:19<00:22,  1.13it/s, loss=0.764]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:19<00:22,  1.13it/s, loss=0.609]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:20<00:21,  1.13it/s, loss=0.609]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:20<00:21,  1.13it/s, loss=0.681]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:21<00:20,  1.13it/s, loss=0.681]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:21<00:20,  1.13it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:22<00:19,  1.13it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:22<00:19,  1.13it/s, loss=0.492]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:23<00:18,  1.13it/s, loss=0.492]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:23<00:18,  1.13it/s, loss=0.701]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:23<00:17,  1.13it/s, loss=0.701]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:24<00:17,  1.13it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:24<00:16,  1.13it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:24<00:16,  1.13it/s, loss=0.386]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:25<00:15,  1.13it/s, loss=0.386]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:25<00:15,  1.13it/s, loss=0.841]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:26<00:15,  1.13it/s, loss=0.841]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:26<00:15,  1.13it/s, loss=0.501]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:27<00:14,  1.13it/s, loss=0.501]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:27<00:14,  1.13it/s, loss=0.573]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:28<00:13,  1.13it/s, loss=0.573]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:28<00:13,  1.13it/s, loss=0.978]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:29<00:12,  1.13it/s, loss=0.978]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:29<00:12,  1.13it/s, loss=0.484]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:30<00:11,  1.13it/s, loss=0.484]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:30<00:11,  1.13it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:31<00:10,  1.13it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:31<00:10,  1.13it/s, loss=0.739]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:31<00:09,  1.13it/s, loss=0.739]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:31<00:09,  1.13it/s, loss=0.688]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:32<00:08,  1.13it/s, loss=0.688]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:32<00:08,  1.13it/s, loss=0.611]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:33<00:07,  1.13it/s, loss=0.611]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:33<00:07,  1.13it/s, loss=0.485]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:34<00:07,  1.13it/s, loss=0.485]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:34<00:07,  1.13it/s, loss=0.878]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:35<00:06,  1.13it/s, loss=0.878]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:35<00:06,  1.13it/s, loss=0.594]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:36<00:05,  1.13it/s, loss=0.594]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:36<00:05,  1.13it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:37<00:04,  1.13it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:37<00:04,  1.13it/s, loss=0.835]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:38<00:03,  1.13it/s, loss=0.835]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:38<00:03,  1.13it/s, loss=0.76] \u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:39<00:02,  1.13it/s, loss=0.76]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:39<00:02,  1.13it/s, loss=0.358]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:39<00:01,  1.13it/s, loss=0.358]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:39<00:01,  1.13it/s, loss=1.22] \u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:40<00:00,  1.13it/s, loss=1.22]\u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:40<00:00,  1.13it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:41<00:00,  1.13it/s, loss=0.638]\u001b[A\n",
      "Train epoch: 100%|██████████| 1/1 [03:56<00:00, 236.23s/it]0,  1.13it/s, loss=0.446]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:56<00:00,  1.06it/s, loss=0.619, dist_mean=0.269]\u001b[A\n",
      "Train epoch: 100%|██████████| 1/1 [03:56<00:00, 236.24s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(use_lora=0)\n",
    "\n",
    "model.save_pretrained('/aiffel/KoChatGPT/output_2_RM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c8be88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 똥멍청이 입니다\n",
      "reward score: -0.6\n"
     ]
    }
   ],
   "source": [
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "input_text = '인공지능은 똥멍청이 입니다'\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68045250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.\n",
      "reward score: -0.6\n"
     ]
    }
   ],
   "source": [
    "input_text = '인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aadf535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05d24ec1",
   "metadata": {},
   "source": [
    "#### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45537a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53c1d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained='/aiffel/KoChatGPT/output_1_SFT', lora_rank=0).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained='/aiffel/KoChatGPT/output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", \n",
    "        model_max_length=512\n",
    "    )\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a81c797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8809236",
   "metadata": {},
   "outputs": [],
   "source": [
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e02e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25be5941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[47311, 10448, 19008,  9792, 11780, 11308, 30190, 10929, 11849, 21663,\n",
      "         44389,  9574, 13799,   458, 14308, 12778, 22469, 20938, 44696,   458,\n",
      "         13799,   458, 14308, 12778, 11756, 18944,   389]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_fn('It takes something more than intelligence to act intelligently.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03c5793d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96c6b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,  \n",
    "                     train_batch_size=8, \n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06ba677d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode [1/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.09s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.000422]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.73it/s, actor_loss=0, critic_loss=0.000422]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.73it/s, actor_loss=0, critic_loss=0.107]   \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=0, critic_loss=0.107]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=0, critic_loss=0.00651]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s, actor_loss=0, critic_loss=0.00651]\u001b[A\n",
      "Episode [1/10]: 100%|██████████| 3/3 [00:19<00:00,  6.59s/it]\n",
      "Episode [2/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.92s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.16, critic_loss=0.0223]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.84it/s, actor_loss=0.16, critic_loss=0.0223]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.84it/s, actor_loss=0.217, critic_loss=0.069]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=0.217, critic_loss=0.069]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=0.151, critic_loss=0.0428]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s, actor_loss=0.151, critic_loss=0.0428]\u001b[A\n",
      "Episode [2/10]: 100%|██████████| 3/3 [00:19<00:00,  6.45s/it]\n",
      "Episode [3/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.96s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.107, critic_loss=0.0138]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.86it/s, actor_loss=0.107, critic_loss=0.0138]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.86it/s, actor_loss=0.104, critic_loss=0.000928]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=0.104, critic_loss=0.000928]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=0.0981, critic_loss=0.0164] \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s, actor_loss=0.0981, critic_loss=0.0164]\u001b[A\n",
      "Episode [3/10]: 100%|██████████| 3/3 [00:19<00:00,  6.49s/it]\n",
      "Episode [4/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.95s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.162, critic_loss=0.028]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.83it/s, actor_loss=-.162, critic_loss=0.028]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.83it/s, actor_loss=-.161, critic_loss=0.0255]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=-.161, critic_loss=0.0255]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=-.154, critic_loss=0.0137]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s, actor_loss=-.154, critic_loss=0.0137]\u001b[A\n",
      "Episode [4/10]: 100%|██████████| 3/3 [00:19<00:00,  6.45s/it]\n",
      "Episode [5/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.82s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0409, critic_loss=0.00134]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.86it/s, actor_loss=-.0409, critic_loss=0.00134]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.86it/s, actor_loss=-.0425, critic_loss=0.00197]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=-.0425, critic_loss=0.00197]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=-.0451, critic_loss=0.00644]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s, actor_loss=-.0451, critic_loss=0.00644]\u001b[A\n",
      "Episode [5/10]: 100%|██████████| 3/3 [00:19<00:00,  6.41s/it]\n",
      "Episode [6/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.86s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.126, critic_loss=0.0173]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.84it/s, actor_loss=0.126, critic_loss=0.0173]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.84it/s, actor_loss=0.137, critic_loss=0.013] \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=0.137, critic_loss=0.013]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=0.126, critic_loss=0.00819]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s, actor_loss=0.126, critic_loss=0.00819]\u001b[A\n",
      "Episode [6/10]: 100%|██████████| 3/3 [00:19<00:00,  6.44s/it]\n",
      "Episode [7/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.90s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0122, critic_loss=0.00099]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.86it/s, actor_loss=0.0122, critic_loss=0.00099]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.86it/s, actor_loss=0.0165, critic_loss=0.000773]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=0.0165, critic_loss=0.000773]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=0.0218, critic_loss=0.00554] \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s, actor_loss=0.0218, critic_loss=0.00554]\u001b[A\n",
      "Episode [7/10]: 100%|██████████| 3/3 [00:19<00:00,  6.44s/it]\n",
      "Episode [8/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.85s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0864, critic_loss=0.00802]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s, actor_loss=-.0864, critic_loss=0.00802]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.85it/s, actor_loss=-.0912, critic_loss=0.0106] \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=-.0912, critic_loss=0.0106]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=-.0894, critic_loss=0.0036]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s, actor_loss=-.0894, critic_loss=0.0036]\u001b[A\n",
      "Episode [8/10]: 100%|██████████| 3/3 [00:19<00:00,  6.39s/it]\n",
      "Episode [9/10]:  67%|██████▋   | 2/3 [00:10<00:05,  5.23s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.023, critic_loss=0.000351]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.84it/s, actor_loss=-.023, critic_loss=0.000351]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.84it/s, actor_loss=-.0271, critic_loss=0.000352]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=-.0271, critic_loss=0.000352]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=-.0295, critic_loss=0.0034]  \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s, actor_loss=-.0295, critic_loss=0.0034]\u001b[A\n",
      "Episode [9/10]: 100%|██████████| 3/3 [00:18<00:00,  6.03s/it]\n",
      "Episode [10/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.85s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0656, critic_loss=0.00473]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s, actor_loss=0.0656, critic_loss=0.00473]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.85it/s, actor_loss=0.0507, critic_loss=0.00383]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=0.0507, critic_loss=0.00383]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=0.0737, critic_loss=0.00341]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s, actor_loss=0.0737, critic_loss=0.00341]\u001b[A\n",
      "Episode [10/10]: 100%|██████████| 3/3 [00:17<00:00,  5.90s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "model.save_pretrained('aiffel/KoChatGPT/output_3_PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d608061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2207c5cb",
   "metadata": {},
   "source": [
    "#### 회고록\n",
    "NLP에도 강화학습을 활용한다니, 그리고 어떻게 보면? 강화학습이 NLP의 한 분야였다는게 신기하다. 그리고 pretrain 모델을 배울수 있었다는 점에서 앞으로도 계속 활용해 볼 수 있을 것 같다. 성능측면에서 보면 아무래도 사전학습이 된 모델이니 당연히 성능이 더 좋을 수 밖에. hyperparameter 및 parameter tuning을 더 시도해보면 더 좋은 성능이 나올 수 있었을까? 이런 생각도 든다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3e5c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f0e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
