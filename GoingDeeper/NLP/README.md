Contents from GoingDeeper, NLP progress.
##CONTENTS
|N|Title|Contents|
|------|------|------|
|1|텍스트 데이터 다루기 <br>Handling Text Data|다양한 텍스트 데이터 전처리 기법을 소개, Word나 형태소 레벨의 tokenizer 및 subword 레벨 tokenizing 기법(BPE, sentencepiece) 학습 <br>Introducing various text data preprocessing techniques, including tokenizers at the word or morpheme level, and subword-level tokenizing techniques (BPE, sentencepiece)|
|2|멋진 단어사전 만들기|[PROJECT] 단어사전을 만들어보고 이를 토대로 perplexity를 측정해보는 프로젝트|
|3|텍스트의 분포로 벡터화 하기|텍스트 분포를 이용한 텍스트의 벡터화 방법들(BoW, DTM, TF-IDF, LSA, LDA)|
|4|뉴스 카테고리 다중분류|[PROJECT] 뉴스 텍스트의 주제를 분류하는 task를 다양한 기법으로 시도해보고 비교, 분석 하는 프로젝트|
|5|워드 임베딩|워드 임베딩 벡터(Word2Vec, FastText, Glove)의 원리와 사용법을 학습|
|6|WEAT|[PROJECT] WEAT(Word Embedding Association Test) 기법으로, Word Embedding Model 의 편향성 측정|
|7|Seq2seq와 Attention|언어 모델이 발전해 온 과정에 대해 배우고, Seq2seq에 대해 학습|
|8|Seq2seq으로 번역기 만들기|Attention 기법을 추가하여 Seq2seq 기반의 번역기 성능을 높여보기|
|9|Transformer가 나오기까지|Attention 복습 및 트랜스포머에 포함된 모듈을 심층적으로 이해하는 단계|
|10|Transformer로 번역기 만들기|트랜스포머를 이용해 번역기를 만드는 프로젝트|
|11|기계 번역이 걸어온 길|자연어 처리에서 Data Augmentation은 어떻게 하는지, 자연어 처리 성능은 어떻게 측정할 수 있는지 학습|
|12|번역가는 대화에도 능하다|다양한 디코딩 방식을 활용해 모델 구현 후 BLEU Score를 이용하여 성능 평가, 한국어 챗봇 구현 프로젝트 수행|
|13|modern NLP의 흐름에 올라타보자|트랜스포머를 바탕으로 한 최근 NLP 모델에 대해 학습|
|14|BERT pretrained model 제작|대표적인 pretrained language model인 BERT 원리에 대해 학습|
|15|NLP Framework의 활용|최다양한 NLP Framework에 대해 학습하고, Huggingface transformer를 중심으로 설계구조와 활용법 학습|
|16|HuggingFace 커스텀 프로젝트 만들기|Huggingface transformer를 활용한 커스텀 프로젝트 수행|
|17|#NLP Trend Note 1|최신 LLM 소개, InstructGPT의 SFT, RM, PPO 학습 메커니즘 소개|
|18|#NLP Trend Note 2|KochatGPT 구현|
