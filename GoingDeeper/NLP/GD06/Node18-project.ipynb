{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ced22eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21.4\n",
      "1.3.3\n",
      "2.6.0\n",
      "3.6.5\n",
      "3.8.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(tf.__version__)\n",
    "print(nltk.__version__)\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "492bfa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==3.8.1 in /opt/conda/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.1) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.1) (1.21.4)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.1) (1.16.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.1) (5.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim==3.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b31a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5be3938f",
   "metadata": {},
   "source": [
    "### Step1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a9dfbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/aiffel/aiffel/workplace/Aiffel_Quest/GoingDeeper/GD06'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from requests import get\n",
    "import os\n",
    "\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890e073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1529457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir('./data')\n",
    "def download(url, file_name):\n",
    "    with open(file_name, \"wb\") as file:   \n",
    "        response = get(url)\n",
    "        file.write(response.content)\n",
    "        \n",
    "url = 'https://raw.githubusercontent.com/songys/Chatbot_data/master'\n",
    "file_name = 'ChatbotData.csv'\n",
    "\n",
    "download(f'{url}/{file_name}', f'./data/{file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c756fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChatbotData.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8ed96c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'./data/ChatbotData.csv')\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdec4555",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_184/1509735071.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "021a8b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7731"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(['Q'], inplace=True)\n",
    "df.drop_duplicates(['A'], inplace=True)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e3386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b07fc0c",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "734e2a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    \n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!가-힣ㄱ-ㅎㅏ-ㅣ0-9]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "600d7b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ppl 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sd카드 망가졌어</td>\n",
       "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Q                   A\n",
       "0        12시 땡!          하루가 또 가네요.\n",
       "1   1지망 학교 떨어졌어           위로해 드립니다.\n",
       "2  3박4일 놀러가고 싶다         여행은 언제나 좋죠.\n",
       "4       ppl 심하네          눈살이 찌푸려지죠.\n",
       "5     sd카드 망가졌어  다시 새로 사는 게 마음 편해요."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Q'] = df['Q'].apply(lambda it: preprocess_sentence(it))\n",
    "df['A'] = df['A'].apply(lambda it: preprocess_sentence(it))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "810e98a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7731, 7731)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = df['Q']\n",
    "answers = df['A']\n",
    "\n",
    "len(questions), len(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f130ca",
   "metadata": {},
   "source": [
    "### Step 3. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ba55e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len : 15\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_corpus(src_data, tgt_data):\n",
    "    \n",
    "    mecab = Mecab()\n",
    "    \n",
    "    def get_morphs(s):\n",
    "        return mecab.morphs(s)\n",
    "    \n",
    "    mecab_src_corpus = list(map(get_morphs, src_data))\n",
    "    mecab_tgt_corpus = list(map(get_morphs, src_data))\n",
    "    \n",
    "    mecab_num_tokens = [len(s) for s in mecab_src_corpus] + [len(s) for s in mecab_tgt_corpus]\n",
    "    \n",
    "    # 최대 길이를 (평균 + 2*표준편차)로 계산\n",
    "    max_len = round(np.mean(mecab_num_tokens) + 2 * np.std(mecab_num_tokens))\n",
    "    print(f'max_len : {max_len}')\n",
    "    \n",
    "    src_corpus, tgt_corpus = [], []\n",
    "    for q, a in zip(mecab_src_corpus, mecab_tgt_corpus):\n",
    "        if len(q) <= max_len and len(a) <= max_len:\n",
    "            if q not in src_corpus and a not in tgt_corpus:\n",
    "                src_corpus.append(q)\n",
    "                tgt_corpus.append(a)\n",
    "    \n",
    "    return src_corpus, tgt_corpus\n",
    "\n",
    "que_corpus, ans_corpus = build_corpus(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b5bb739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7429, 7429)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(que_corpus), len(ans_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912422e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4135b398",
   "metadata": {},
   "source": [
    "### Step4. Augmentation\n",
    "\n",
    "Lexical Substitution을 실제로 적용해보자. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c041aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec.load('/aiffel/aiffel/transformer_chatbot/data/word2vec_ko.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f9baf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def lexical_sub(sentence, word2vec):\n",
    "    try:\n",
    "        _from = random.choice(sentence)\n",
    "        _to = word2vec.wv.most_similar(_from)[0][0]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    res = []\n",
    "    for x in sentence:\n",
    "        if x is _from: res.append(_to)\n",
    "        else: res.append(x)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "557aee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "def augment_corpus(src_corpus, tgt_corpus, wv):\n",
    "    new_src_corpus = []\n",
    "    new_tar_corpus = []\n",
    "    corpus_size = len(src_corpus)\n",
    "    \n",
    "    for i in tqdm_notebook(range(corpus_size)):\n",
    "        q = src_corpus[i]\n",
    "        a = tgt_corpus[i]\n",
    "        \n",
    "        new_src = lexical_sub(q, wv)\n",
    "        new_tar = lexical_sub(a, wv)\n",
    "        \n",
    "        if new_src: \n",
    "            new_src_corpus.append(new_src)\n",
    "            new_tar_corpus.append(a)\n",
    "            \n",
    "        if new_tar: \n",
    "            new_src_corpus.append(q)\n",
    "            new_tar_corpus.append(new_tar)\n",
    "    \n",
    "    return new_src_corpus, new_tar_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a00763e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1664475964.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(range(corpus_size)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170ef61fe5c5429f9c86afa2db53e325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7429 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(22057, 22057)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_q, aug_a = augment_corpus(que_corpus, ans_corpus, w2v_model)\n",
    "\n",
    "que_corpus += aug_q\n",
    "ans_corpus += aug_a\n",
    "    \n",
    "len(que_corpus), len(ans_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a525f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c892c1e2",
   "metadata": {},
   "source": [
    "### Step5. 데이터 벡터화\n",
    "\n",
    "ans_corpus에 <start>토큰과 <end> 토큰이 추가되지 않은 상태이니 이를 먼저 해결한 후 벡터화를 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb08556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_corpus = [[\"<start>\"] + ans + [\"<end>\"] for ans in ans_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "382444ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  \n",
    "\n",
    "\n",
    "data = que_corpus + ans_corpus\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=None, filters=' ', oov_token='<unk>')\n",
    "tokenizer.fit_on_texts(data)\n",
    "\n",
    "enc_train = tokenizer.texts_to_sequences(que_corpus)\n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(enc_train, padding='post')\n",
    "\n",
    "dec_train = tokenizer.texts_to_sequences(ans_corpus)\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(dec_train, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19ba89a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22057, 15), (22057, 17))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train.shape, dec_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e10dc48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5903"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.index_word) + 2\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc89646d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd19b488",
   "metadata": {},
   "source": [
    "### Step 6. 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e91ece1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    \n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i)/d_model)  # np.power(a,b) > a^b(제곱)\n",
    "    \n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "    \n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    \n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba769771",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)  \n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        \n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_qk += (mask * -1e9)\n",
    "        \n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "\n",
    "        out = tf.matmul(attentions, V)\n",
    "        return out, attentions\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "        return split_x\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "        return combined_x\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask\n",
    "        )\n",
    "        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b08ecda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5bd48a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        \n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dde4eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3d9ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "            \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0e2684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        \n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e915a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, src_vocab_size, tgt_vocab_size,\n",
    "                 pos_len, dropout=0.2, shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        \n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        \n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "        \n",
    "        self.shared = shared\n",
    "        \n",
    "        if shared:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "        \n",
    "        \n",
    "    def embedding(self, emb, x):\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "        \n",
    "        if self.shared:\n",
    "            out *= tf.math.sqrt(self.d_model)\n",
    "        \n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00c27dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6596060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe16b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58be15ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 주어진 하이퍼파라미터로 Transformer 인스턴스 생성\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    # shared_fc=True,\n",
    "    # shared_emb=True\n",
    "    shared=True)\n",
    "\n",
    "d_model = 512\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68a30d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate 인스턴스 선언 & Optimizer 구현\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)\n",
    "\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16c419e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Loss Function 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "97553c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d3fd508b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/4012230158.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtqdm_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5 \n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    tqdm_bar = tqdm(total=len(train_dataset))\n",
    "\n",
    "    for (batch, (src, tgt)) in enumerate(train_dataset):\n",
    "        tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "        gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "        enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, _, _, _ = transformer(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "            loss = loss_function(gold, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "        total_loss += loss\n",
    "        tqdm_bar.update(1)\n",
    "        tqdm_bar.set_description(f'Epoch {epoch + 1}, Loss: {total_loss / (batch + 1):.4f}')\n",
    "\n",
    "    tqdm_bar.close()\n",
    "\n",
    "    # 검증 데이터셋을 사용하여 에포크 종료 후 모델 성능 평가 가능\n",
    "\n",
    "    # 에포크 종료 후 손실 출력\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_dataset):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d99fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ca2726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec5c962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "class ChatBot():\n",
    "    def __init__(self, tokenizer, enc_data, dec_data,\n",
    "                 n_layers=6, d_model=512, n_heads=8, d_ff=2048,\n",
    "                 vocab_size=20000, pos_len=200, dropout=0.3, shared=True,\n",
    "                 epochs=20, batch_size=64):\n",
    "        super(ChatBot, self).__init__()\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.enc_data = enc_data\n",
    "        self.dec_data = dec_data\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.model = Transformer(\n",
    "            n_layers=n_layers,\n",
    "            d_model=d_model,\n",
    "            n_heads=n_heads,\n",
    "            d_ff=d_ff,\n",
    "            src_vocab_size=vocab_size,\n",
    "            tgt_vocab_size=vocab_size,\n",
    "            pos_len=pos_len,\n",
    "            dropout=dropout,\n",
    "            shared=shared\n",
    "        )\n",
    "        \n",
    "        self.EPOCHS = epochs\n",
    "        self.BATCH_SIZE = batch_size\n",
    "        \n",
    "        learning_rate = LearningRateScheduler(d_model)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "        self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "        \n",
    "        self.examples = [\n",
    "            \"말하기 전에 생각했나요? 입에서 때가 나오네요.\",\n",
    "            \"논문은 쓰셨나요? \",\n",
    "            \"무슨 영화를 제일 좋아하세요? 부귀영화요.\",\n",
    "            \"굳이? 구지? 구찌.\"\n",
    "        ]\n",
    "        \n",
    "    def loss_function(self, real, pred):\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "        loss_ = self.loss_object(real, pred)\n",
    "        \n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "\n",
    "        return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "    \n",
    "    \n",
    "    @tf.function()\n",
    "    def train_step(self, src, tgt):\n",
    "        gold = tgt[:, 1:]\n",
    "\n",
    "        enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, enc_attns, dec_attns, dec_enc_attns = self.model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "            loss = self.loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)    \n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "        return loss, enc_attns, dec_attns, dec_enc_attns\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        for epoch in range(self.EPOCHS):\n",
    "            total_loss = 0\n",
    "\n",
    "            idx_list = list(range(0, self.enc_data.shape[0], self.BATCH_SIZE))   \n",
    "            random.shuffle(idx_list)\n",
    "            t = tqdm(idx_list)\n",
    "            \n",
    "            for (batch, idx) in enumerate(t):\n",
    "                batch_loss, enc_attns, dec_attns, dec_enc_attns = self.train_step(\n",
    "                    self.enc_data[idx:idx+self.BATCH_SIZE],\n",
    "                    self.dec_data[idx:idx+self.BATCH_SIZE])\n",
    "                \n",
    "                total_loss += batch_loss\n",
    "\n",
    "                t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "                t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "                \n",
    "                \n",
    "    def evaluate(self, sentence):\n",
    "        mecab = Mecab()\n",
    "        sentence = mecab.morphs(preprocess_sentence(sentence))\n",
    "        sentence = self.tokenizer.texts_to_sequences(sentence)\n",
    "        _input = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            sentence, maxlen=self.enc_data.shape[-1], padding='post')\n",
    "\n",
    "        ids = []\n",
    "        output = tf.expand_dims([self.tokenizer.word_index['<start>']], 0)\n",
    "        for i in range(self.dec_data.shape[-1]):\n",
    "            enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(_input, output)\n",
    "\n",
    "            predictions, enc_attns, dec_attns, dec_enc_attns = self.model(\n",
    "                _input, output, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "            \n",
    "            predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "            \n",
    "            if tokenizer.word_index['<end>'] == predicted_id:\n",
    "                result = ' '.join(self.tokenizer.sequences_to_texts([ids]))\n",
    "                return result, enc_attns, dec_attns, dec_enc_attns\n",
    "            \n",
    "            ids.append(predicted_id)\n",
    "            output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "    \n",
    "        result = ' '.join(self.tokenizer.sequences_to_texts([ids]))\n",
    "        return result, enc_attns, dec_attns, dec_enc_attns\n",
    "        \n",
    "        \n",
    "    def chat(self, sentence):\n",
    "        result, enc_attns, dec_attns, dec_enc_attns = self.evaluate(sentence)\n",
    "        \n",
    "        print(f'Q: {sentence}')\n",
    "        print(f'A: {result}')\n",
    "       \n",
    "        \n",
    "    def eval_bleu_single(self, src_sentence, tgt_sentence):\n",
    "        src_tokens = self.tokenizer.texts_to_sequences(src_sentence)\n",
    "        tgt_tokens = self.tokenizer.texts_to_sequences(tgt_sentence)\n",
    "\n",
    "        if (len(src_tokens) > self.enc_data.shape[-1]): return None\n",
    "        if (len(tgt_tokens) > self.enc_data.shape[-1]): return None\n",
    "\n",
    "        reference = tgt_sentence.split()\n",
    "        candidate, _, _, _ = self.evaluate(src_sentence)\n",
    "        \n",
    "        score = sentence_bleu([reference], candidate,\n",
    "                              smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "        return score\n",
    "\n",
    "\n",
    "    def eval_bleu(self, src_sentences, tgt_sentence):\n",
    "        total_score = 0.0\n",
    "        sample_size = len(src_sentences)\n",
    "\n",
    "        for idx in tqdm(range(sample_size)):\n",
    "            score = self.eval_bleu_single(src_sentences[idx], tgt_sentence[idx])\n",
    "            if not score: continue\n",
    "\n",
    "            total_score += score\n",
    "            \n",
    "        return total_score / sample_size\n",
    "        \n",
    "    \n",
    "    def submit(self, src_sentences, tgt_sentence):\n",
    "        print('# 제출')\n",
    "        print()\n",
    "        \n",
    "        print('Translations')\n",
    "        for i, s in enumerate(self.examples):\n",
    "            print(f'> {i + 1}')\n",
    "            self.chat(s)\n",
    "            print()\n",
    "            \n",
    "        print('BLEU Score')\n",
    "        print(f'score: {self.eval_bleu(src_sentences, tgt_sentence)}')\n",
    "        print()\n",
    "\n",
    "        print('Hyperparameters')\n",
    "        print(f'> n_layers: {self.n_layers}')\n",
    "        print(f'> d_model: {self.d_model}')\n",
    "        print(f'> n_heads: {self.n_heads}')\n",
    "        print(f'> d_ff: {self.d_ff}')\n",
    "        print(f'> dropout: {self.dropout}')\n",
    "        print()\n",
    "        \n",
    "        print('Training Parameters')\n",
    "        print(f'> Batch Size: {self.BATCH_SIZE}')\n",
    "        print(f'> Epoch At: {self.EPOCHS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230969f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c46d3e5",
   "metadata": {},
   "source": [
    "### Step 7. 성능 측정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "132d8add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7fefb626704a1e89522cff5a7670d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283f9701c88b415c8a535531919c698c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0fc7ad038d47f180f199ccbfdbf1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767ce391450348e2966bd97def9e8df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c3d30bc9694985be997e7568909a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d90f3ac53647e286cf160463de21c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e68c764cae46ca8d02c669b56750a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004cd5d90f9a4e39a42c315ab63ee539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9f7306f7a547ef97b44a9638fbc387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f78f63cf32740b0b5cdab982494b245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2955f8bd03b74e2e9d32831d02c7a1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af30f08533e40b7ad0194260756ccb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea41d79a6e2242ea8ebc4660a2cab92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393d8b05154b4a369b780e18ca8a764d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94ecb762b504c05b649f3252459a939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d66944db6847b5914a57e5a778b55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a35ae48adeb4886a5bb2ea4690a8405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ce58f507e94332bb1f2a26b58b6062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c416252cf05e4388ad749dad973360ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ec6fd832174d578b04e6a2122926ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_1 = ChatBot(tokenizer, enc_train, dec_train)\n",
    "model_1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "89398104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 제출\n",
      "\n",
      "Translations\n",
      "> 1\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 고백\n",
      "\n",
      "> 2\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 오늘 오늘 오늘 오늘 오늘 오늘 오늘 오늘\n",
      "\n",
      "> 3\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 간만에\n",
      "\n",
      "> 4\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 집 집 집\n",
      "\n",
      "BLEU Score\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81dca3df2284e73a3975bffacd1f0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.0\n",
      "\n",
      "Hyperparameters\n",
      "> n_layers: 6\n",
      "> d_model: 512\n",
      "> n_heads: 8\n",
      "> d_ff: 2048\n",
      "> dropout: 0.3\n",
      "\n",
      "Training Parameters\n",
      "> Batch Size: 64\n",
      "> Epoch At: 20\n"
     ]
    }
   ],
   "source": [
    "model_1.submit(list(questions[:100]), list(answers[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3d8cfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975384ef5d4b496da115e8328e4d4adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d01c9389db64d55bd744b231df28491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7625199cde842fcafed389361db0a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb81852eb5c04c649e16526b92a742cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce96743ed46414792f71df68386fbb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78655fca92c24e1e869f0616b23ee28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a02895a48e542d098944c908f560b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb11b586717e469991fd0b61a35fe1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54688925b9da494bab0dcb32ed59c048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5200f01ad4264395928b1826c605d29a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd64c4dbf4f641fd9508d4336e237308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e64a163af040b2b611a454f50e56ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6db64cb10144377ad051726163d648b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f18e891e2fa45268d624a5055c98a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6014d48da05b42c29b17766d914c3af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e9df5fc68f4781ac31f88f334f18a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1b685617ff4dd2b98eb1153e9603ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2854ff360754cb1a1cfae9a0a37b705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dab850504bd4a1d8224a31a3c503cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98092a196aa44c0990eb97cc361f8a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_2 = ChatBot(tokenizer, enc_train, dec_train, n_layers=6, n_heads=16, d_model=1024, d_ff=4096)\n",
    "model_2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d4ac706a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 제출\n",
      "\n",
      "Translations\n",
      "> 1\n",
      "Q: 지루하다, 놀러가고 싶어.\n",
      "A: 지루 지루 낮잠 지루\n",
      "\n",
      "> 2\n",
      "Q: 오늘 일찍 일어났더니 피곤하다.\n",
      "A: 오늘 피곤\n",
      "\n",
      "> 3\n",
      "Q: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "A: 간만에 간만에 간만에\n",
      "\n",
      "> 4\n",
      "Q: 집에 있는다는 소리야.\n",
      "A: 집 힘내\n",
      "\n",
      "BLEU Score\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc281c3a740451f97a21a408b96669e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.0\n",
      "\n",
      "Hyperparameters\n",
      "> n_layers: 6\n",
      "> d_model: 1024\n",
      "> n_heads: 16\n",
      "> d_ff: 4096\n",
      "> dropout: 0.3\n",
      "\n",
      "Training Parameters\n",
      "> Batch Size: 64\n",
      "> Epoch At: 20\n"
     ]
    }
   ],
   "source": [
    "model_2.submit(list(questions[:100]), list(answers[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31236c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac15de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
